{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29594,"databundleVersionId":2408861,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# All needed libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_absolute_error\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nimport gc ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:07.753965Z","iopub.execute_input":"2025-10-07T17:48:07.754224Z","iopub.status.idle":"2025-10-07T17:48:18.418992Z","shell.execute_reply.started":"2025-10-07T17:48:07.754202Z","shell.execute_reply":"2025-10-07T17:48:18.418405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', None)","metadata":{"_uuid":"c271b2b8-40b3-4a24-b9fa-ba7c2bd96a52","_cell_guid":"9a24d2bc-70ac-4cfd-8c40-e3e2605ece2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-10-07T17:48:18.420181Z","iopub.execute_input":"2025-10-07T17:48:18.421063Z","iopub.status.idle":"2025-10-07T17:48:18.426041Z","shell.execute_reply.started":"2025-10-07T17:48:18.421040Z","shell.execute_reply":"2025-10-07T17:48:18.425476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dtypes_train = {'id': 'int32',\n          'breath_id': 'int32',\n          'R' : 'int8',\n          'C' : 'int8',\n          'time_step': 'float64',\n          'u_in': 'float64',\n          'u_out': 'int8',\n          'pressure': 'float64'}\n\ndtypes_test = {'id': 'int32',\n          'breath_id': 'int32',\n          'R' : 'int8',\n          'C' : 'int8',\n          'time_step': 'float64',\n          'u_in': 'float64',\n          'u_out': 'int8'}\n\ndef read_train():\n    train = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/train.csv')\n    train = train.astype(dtypes_train)\n    return train\n\ndef read_test():\n    test = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n    test = test.astype(dtypes_test)\n    return test  \n\ntrain = read_train()\ntest = read_test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:18.426616Z","iopub.execute_input":"2025-10-07T17:48:18.426831Z","iopub.status.idle":"2025-10-07T17:48:30.231808Z","shell.execute_reply.started":"2025-10-07T17:48:18.426806Z","shell.execute_reply":"2025-10-07T17:48:30.231224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/sample_submission.csv')\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\nprint(\"Sample submission shape:\", sample_submission.shape)\n\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:30.233049Z","iopub.execute_input":"2025-10-07T17:48:30.233282Z","iopub.status.idle":"2025-10-07T17:48:30.948959Z","shell.execute_reply.started":"2025-10-07T17:48:30.233256Z","shell.execute_reply":"2025-10-07T17:48:30.948201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train.head(200000).describe().round(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:30.949779Z","iopub.execute_input":"2025-10-07T17:48:30.950055Z","iopub.status.idle":"2025-10-07T17:48:31.020074Z","shell.execute_reply.started":"2025-10-07T17:48:30.950029Z","shell.execute_reply":"2025-10-07T17:48:31.019440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train.dtypes)\n\nprint(\"\\nUnique values of breath_id:\", train['breath_id'].nunique())\n\nprint(\"R values:\", train['R'].unique())\nprint(\"C values:\", train['C'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:31.020739Z","iopub.execute_input":"2025-10-07T17:48:31.020914Z","iopub.status.idle":"2025-10-07T17:48:31.107448Z","shell.execute_reply.started":"2025-10-07T17:48:31.020900Z","shell.execute_reply":"2025-10-07T17:48:31.106658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example = train[train['breath_id'] == 500]\n\nplt.figure(figsize=(12,6))\nplt.plot(example['time_step'], example['u_in'], label='u_in (valve control)')\nplt.plot(example['time_step'], example['pressure'], label='pressure')\nplt.plot(example['time_step'], example['u_out'], label='u_out (exhalation phase)')\nplt.xlabel(\"time_step\")\nplt.ylabel(\"Value\")\nplt.title(\"Example 500 of a single breath\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:31.108393Z","iopub.execute_input":"2025-10-07T17:48:31.108661Z","iopub.status.idle":"2025-10-07T17:48:31.356571Z","shell.execute_reply.started":"2025-10-07T17:48:31.108635Z","shell.execute_reply":"2025-10-07T17:48:31.355866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(data=train, x='pressure', kde=True, bins=50)\nplt.title('Distribution of Target Variable (Pressure)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T18:10:38.483372Z","iopub.execute_input":"2025-10-07T18:10:38.483628Z","iopub.status.idle":"2025-10-07T18:10:59.131362Z","shell.execute_reply.started":"2025-10-07T18:10:38.483609Z","shell.execute_reply":"2025-10-07T18:10:59.130736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_breaths = train['breath_id'].unique()[0:5]\n\nfor breath_id in sample_breaths:\n    breath_df = train[train['breath_id'] == breath_id]\n    \n    plt.figure(figsize=(12, 5))\n    plt.plot(breath_df['time_step'], breath_df['u_in'], label='u_in (Inspiratory Flow)')\n    plt.plot(breath_df['time_step'], breath_df['u_out'], label='u_out (Expiratory Phase)')\n    plt.plot(breath_df['time_step'], breath_df['pressure'], label='Pressure (Target)', linestyle='--')\n    \n    plt.title(f'Profile for Breath ID: {breath_id}')\n    plt.xlabel('Time Step')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T18:11:04.270040Z","iopub.execute_input":"2025-10-07T18:11:04.270290Z","iopub.status.idle":"2025-10-07T18:11:05.210033Z","shell.execute_reply.started":"2025-10-07T18:11:04.270270Z","shell.execute_reply":"2025-10-07T18:11:05.209271Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. First simple submission","metadata":{}},{"cell_type":"markdown","source":"# Submitting mean","metadata":{}},{"cell_type":"code","source":"mean_pressure = train.loc[train['u_out'] == 0, 'pressure'].mean()\n\nsample_submission['pressure'] = mean_pressure\n\nsample_submission.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Mean pressure used for prediction:\", mean_pressure)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T12:15:43.788991Z","iopub.execute_input":"2025-10-07T12:15:43.789529Z","iopub.status.idle":"2025-10-07T12:15:51.127986Z","shell.execute_reply.started":"2025-10-07T12:15:43.789492Z","shell.execute_reply":"2025-10-07T12:15:51.126891Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Linear regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ntrain_filtered = train[train['u_out'] == 0]\n\nfeatures = ['u_in', 'time_step', 'R', 'C']\nX_train = train_filtered[features]\ny_train = train_filtered['pressure']\n\nX_test = test[features]\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nsample_submission['pressure'] = y_pred\nsample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T12:11:07.661890Z","iopub.execute_input":"2025-10-07T12:11:07.662536Z","iopub.status.idle":"2025-10-07T12:11:15.811617Z","shell.execute_reply.started":"2025-10-07T12:11:07.662512Z","shell.execute_reply":"2025-10-07T12:11:15.810867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"def add_feats(train):\n    # # rewritten calculation of lag features from this notebook: https://www.kaggle.com/patrick0302/add-lag-u-in-as-new-feat\n# # some of ideas from this notebook: https://www.kaggle.com/mst8823/google-brain-lightgbm-baseline\n    # train[[\"15_out_mean\"]] = train.groupby('breath_id')['u_out'].rolling(window=15,min_periods=1).agg({\"15_out_mean\":\"mean\"}).reset_index(level=0,drop=True)\n    train['last_value_u_in'] = train.groupby('breath_id')['u_in'].transform('last')\n    train['u_in_lag1'] = train.groupby('breath_id')['u_in'].shift(1)\n    train['u_out_lag1'] = train.groupby('breath_id')['u_out'].shift(1)\n    train['u_in_lag_back1'] = train.groupby('breath_id')['u_in'].shift(-1)\n    train['u_out_lag_back1'] = train.groupby('breath_id')['u_out'].shift(-1)\n    train['u_in_lag2'] = train.groupby('breath_id')['u_in'].shift(2)\n    train['u_out_lag2'] = train.groupby('breath_id')['u_out'].shift(2)\n    train['u_in_lag3'] = train.groupby('breath_id')['u_in'].shift(3)\n    train['u_out_lag3'] = train.groupby('breath_id')['u_out'].shift(3)\n    train['u_in_lag_back2'] = train.groupby('breath_id')['u_in'].shift(-2)\n    train['u_out_lag_back2'] = train.groupby('breath_id')['u_out'].shift(-2)\n    train['u_in_lag_back3'] = train.groupby('breath_id')['u_in'].shift(-3)\n    train['u_out_lag_back3'] = train.groupby('breath_id')['u_out'].shift(-3)\n    train['u_in_lag_back10'] = train.groupby('breath_id')['u_in'].shift(-10)\n    train['u_out_lag_back10'] = train.groupby('breath_id')['u_out'].shift(-10)\n\n    ## time since last step\n    train['time_step_diff'] = train.groupby('breath_id')['time_step'].diff().fillna(0)\n    ### rolling window ts feats\n    train['ewm_u_in_mean'] = train.groupby('breath_id')['u_in'].ewm(halflife=9).mean().reset_index(level=0,drop=True)\n    train['ewm_u_in_std'] = train.groupby('breath_id')['u_in'].ewm(halflife=10).std().reset_index(level=0,drop=True) ## could add covar?\n    train['ewm_u_in_corr'] = train.groupby('breath_id')['u_in'].ewm(halflife=15).corr().reset_index(level=0,drop=True) # self umin corr\n    ## rolling window of 15 periods\n    train[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\",\"15_out_std\"]] = train.groupby('breath_id')['u_in'].rolling(window=15,min_periods=1).agg({\"15_in_sum\":\"sum\",\"15_in_min\":\"min\",\"15_in_max\":\"max\",\"15_in_mean\":\"mean\",\"15_in_std\":\"std\"}).reset_index(level=0,drop=True)\n    train[[\"45_in_sum\",\"45_in_min\",\"45_in_max\",\"45_in_mean\",\"45_out_std\"]] = train.groupby('breath_id')['u_in'].rolling(window=45,min_periods=1).agg({\"45_in_sum\":\"sum\",\"45_in_min\":\"min\",\"45_in_max\":\"max\",\"45_in_mean\":\"mean\",\"45_in_std\":\"std\"}).reset_index(level=0,drop=True)\n\n    train[[\"15_out_mean\"]] = train.groupby('breath_id')['u_out'].rolling(window=15,min_periods=1).agg({\"15_out_mean\":\"mean\"}).reset_index(level=0,drop=True)\n\n    print(train.shape[0])\n    display(train)\n    train = train.fillna(0) # ORIG\n\n    # max, min, mean value of u_in and u_out for each breath\n    train['breath_id__u_in__max'] = train.groupby(['breath_id'])['u_in'].transform('max')\n\n    train['breath_id__u_in__mean'] =train.groupby(['breath_id'])['u_in'].mean()\n\n    train['breath_id__u_in__min'] = train.groupby(['breath_id'])['u_in'].transform('min')\n\n    train['R_div_C'] = train[\"R\"].div(train[\"C\"])\n\n    # difference between consequitive values\n    train['R__C'] = train[\"R\"].astype(str) + '__' + train[\"C\"].astype(str)\n    train['u_in_diff1'] = train['u_in'] - train['u_in_lag1']\n    train['u_out_diff1'] = train['u_out'] - train['u_out_lag1']\n    train['u_in_diff2'] = train['u_in'] - train['u_in_lag2']\n    train['u_out_diff2'] = train['u_out'] - train['u_out_lag2']\n    train['u_in_diff3'] = train['u_in'] - train['u_in_lag3']\n    train['u_out_diff3'] = train['u_out'] - train['u_out_lag3']\n    ## diff between last 2 steps\n    train['u_in_diff_1_2'] = train['u_in_lag1'] - train['u_in_lag2']\n    train['u_out_diff_1_2'] = train['u_out_lag1'] - train['u_out_lag2']\n    train['u_in_lagback_diff_1_2'] = train['u_in_lag_back1'] - train['u_in_lag_back2']\n    train['u_out_lagback_diff_1_2'] = train['u_out_lag_back1'] - train['u_out_lag_back2']\n\n    train['u_in_lagback_diff1'] = train['u_in'] - train['u_in_lag_back1']\n    train['u_out_lagback_diff1'] = train['u_out'] - train['u_out_lag_back1']\n    train['u_in_lagback_diff2'] = train['u_in'] - train['u_in_lag_back2']\n    train['u_out_lagback_diff2'] = train['u_out'] - train['u_out_lag_back2']\n\n    # from here: https://www.kaggle.com/yasufuminakama/ventilator-pressure-lstm-starter\n    # setting the differences to 0 for the first time\n    train.loc[train['time_step'] == 0, 'u_in_diff'] = 0\n    train.loc[train['time_step'] == 0, 'u_out_diff'] = 0\n\n    # difference between the current value of u_in and the max value within the breath\n    train['breath_id__u_in__diffmax'] = train.groupby(['breath_id'])['u_in'].transform('max') - train['u_in']\n    train['breath_id__u_in__diffmean'] = train.groupby(['breath_id'])['u_in'].transform('mean') - train['u_in']\n\n    print(\"before OHE\")\n    display(train)\n\n    # OHE\n    train = train.merge(pd.get_dummies(train['R'], prefix='R'), left_index=True, right_index=True).drop(['R'], axis=1)\n    train = train.merge(pd.get_dummies(train['C'], prefix='C'), left_index=True, right_index=True).drop(['C'], axis=1)\n    train = train.merge(pd.get_dummies(train['R__C'], prefix='R__C'), left_index=True, right_index=True).drop(['R__C'], axis=1)\n\n    # https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/273974\n    train['u_in_cumsum'] = train.groupby(['breath_id'])['u_in'].cumsum()\n    train['time_step_cumsum'] = train.groupby(['breath_id'])['time_step'].cumsum()\n\n    # feature by u in or out (ideally - make 2 sep columns for each state) # dan\n    train['u_in_partition_out_sum'] = train.groupby(['breath_id',\"u_out\"])['u_in'].transform(\"sum\")\n\n    train = train.fillna(0) # add for consistency with how test is done - dan\n\n    return train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:39.546240Z","iopub.execute_input":"2025-10-07T17:48:39.546894Z","iopub.status.idle":"2025-10-07T17:48:39.562511Z","shell.execute_reply.started":"2025-10-07T17:48:39.546870Z","shell.execute_reply":"2025-10-07T17:48:39.561721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adding new features into the dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/ventilator-pressure-prediction/train.csv\")\ntest = pd.read_csv(\"../input/ventilator-pressure-prediction/test.csv\")\n\nprint(\"Engineering features for training data...\")\ntrain = add_feats(train)\n\nprint(\"Engineering features for test data...\")\ntest = add_feats(test) \n\ncolumns = [col for col in train.columns if col not in ['id', 'breath_id', 'pressure']]\ntarget = train[\"pressure\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:48:42.590358Z","iopub.execute_input":"2025-10-07T17:48:42.590641Z","iopub.status.idle":"2025-10-07T17:50:40.810515Z","shell.execute_reply.started":"2025-10-07T17:48:42.590618Z","shell.execute_reply":"2025-10-07T17:50:40.809684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"n_splits = 2\nscores = []\nall_train_losses = []\nall_val_losses = []\nfolds = GroupKFold(n_splits=n_splits)\n\nfor fold, (train_idx, val_idx) in enumerate(folds.split(train, target, groups=train['breath_id'])):\n    print(f\"--- Fold {fold} ---\")\n    \n    X_train, y_train = train.loc[train_idx, columns], target.loc[train_idx]\n    X_val, y_val = train.loc[val_idx, columns], target.loc[val_idx]\n    \n    model = lgb.LGBMRegressor(\n        objective='regression_l1',\n        n_estimators=500,\n        learning_rate=0.05,\n        n_jobs=-1,\n        random_state=42\n    )\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train), (X_val, y_val)],\n        eval_metric='mae',\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100, verbose=False),\n            lgb.log_evaluation(period=100) \n        ]\n    )\n    results = model.evals_result_\n    all_train_losses.append(results['training']['l1'])\n    all_val_losses.append(results['valid_1']['l1'])\n    \n    preds = model.predict(X_val)\n    score = mean_absolute_error(y_val, preds)\n    scores.append(score)\n    print(f\"Fold {fold} MAE: {score:.4f}\\n\")\n\nprint(f\"Average CV MAE with Engineered Features: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n\nmean_train_loss = np.mean(all_train_losses, axis=0)\nmean_val_loss = np.mean(all_val_losses, axis=0)\n\nplt.figure(figsize=(12, 7))\nplt.plot(mean_train_loss, label='Average Training Loss')\nplt.plot(mean_val_loss, label='Average Validation Loss')\nplt.title(f'Average Training & Validation Loss Across All Folds (1000 Rounds)')\nplt.xlabel('Boosting Round')\nplt.ylabel('Loss (MAE)')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T18:04:42.560274Z","iopub.execute_input":"2025-10-07T18:04:42.560567Z","iopub.status.idle":"2025-10-07T18:10:26.529253Z","shell.execute_reply.started":"2025-10-07T18:04:42.560542Z","shell.execute_reply":"2025-10-07T18:10:26.528508Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MLP in PyTorch","metadata":{}},{"cell_type":"code","source":"# Setup device \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# PyTorch Dataset\nclass VentilatorDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = torch.tensor(features, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx].unsqueeze(-1)\n\nclass CustomLinearLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        return x @ self.weight + self.bias\n\n# MLP Model \nclass MLP(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.layers = nn.Sequential(\n            CustomLinearLayer(input_dim, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Dropout(0.2),\n            nn.Linear(64, 1) \n        )\n\n    def forward(self, x):\n        return self.layers(x)\n                \nn_splits = 2\nscores = []\nfolds = GroupKFold(n_splits=n_splits)\nall_train_losses = []\nall_val_losses = []\n\nBATCH_SIZE = 512\nEPOCHS = 5\nLEARNING_RATE = 1e-3\nPATIENCE = 10 # early stopping\n\nfor fold, (train_idx, val_idx) in enumerate(folds.split(train, target, groups=train['breath_id'])):\n    print(f\"--- Fold {fold} ---\")\n    \n    # Split data\n    X_train, y_train = train.loc[train_idx, columns], target.iloc[train_idx]\n    X_val, y_val = train.loc[val_idx, columns], target.iloc[val_idx]\n    \n    # Scale features (training data)\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n\n    # datasets and dataLoaders\n    train_dataset = VentilatorDataset(X_train, y_train.values)\n    val_dataset = VentilatorDataset(X_val, y_val.values)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True, \n        num_workers=2, \n        pin_memory=True\n    )\n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        num_workers=2, \n        pin_memory=True\n    )\n    \n    # model, loss and optimizer\n    model = MLP(input_dim=X_train.shape[1]).to(device)\n    criterion = nn.L1Loss() # MAE Loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train_losses_fold = []\n    val_losses_fold = []\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss = 0.0\n        for features, labels in train_loader:\n            features, labels = features.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for features, labels in val_loader:\n                features, labels = features.to(device), labels.to(device)\n                outputs = model(features)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n\n        train_losses_fold.append(avg_train_loss)\n        val_losses_fold.append(avg_val_loss)\n        \n        print(f\"Epoch {epoch+1}/{EPOCHS}, Train MAE: {avg_train_loss:.4f}, Val MAE: {avg_val_loss:.4f}\")\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                print(\"Early stopping triggered.\")\n                break\n                \n    all_train_losses.append(train_losses_fold)\n    all_val_losses.append(val_losses_fold)\n    \n    scores.append(best_val_loss)\n    print(f\"Fold {fold} Best MAE: {best_val_loss:.4f}\\n\")\n    \n    # memory cleaning\n    del model, X_train, y_train, X_val, y_val, train_loader, val_loader\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(f\"Average CV MAE with PyTorch MLP: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n\nmean_train_loss = np.mean(all_train_losses, axis=0)\nmean_val_loss = np.mean(all_val_losses, axis=0)\n\nplt.figure(figsize=(12, 7))\nplt.plot(mean_train_loss, label='Average Training Loss')\nplt.plot(mean_val_loss, label='Average Validation Loss')\nplt.title('Average Training & Validation Loss Across All Folds')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MAE)')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T18:12:15.996908Z","iopub.execute_input":"2025-10-07T18:12:15.997199Z","iopub.status.idle":"2025-10-07T18:33:08.147138Z","shell.execute_reply.started":"2025-10-07T18:12:15.997178Z","shell.execute_reply":"2025-10-07T18:33:08.146335Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM in PyTorch","metadata":{}},{"cell_type":"markdown","source":"## Training a model to define the most important features","metadata":{}},{"cell_type":"code","source":"X = train[columns]\ny = target\n\nprint(\"Training a baseline model to get feature importances...\")\nbaseline_model = lgb.LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.1,\n    random_state=42,\n    n_jobs=-1\n)\n\nbaseline_model.fit(X, y)\nprint(\"Baseline model training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:09:54.133355Z","iopub.execute_input":"2025-10-07T17:09:54.133692Z","iopub.status.idle":"2025-10-07T17:12:20.481548Z","shell.execute_reply.started":"2025-10-07T17:09:54.133665Z","shell.execute_reply":"2025-10-07T17:12:20.480926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importance_df = pd.DataFrame({\n    'feature': X.columns,\n    'importance': baseline_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 10 most important features:\")\ndisplay(feature_importance_df.head(10))\n\nprint(\"\\nTop 10 least important features:\")\ndisplay(feature_importance_df.tail(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:12:35.027095Z","iopub.execute_input":"2025-10-07T17:12:35.027424Z","iopub.status.idle":"2025-10-07T17:12:35.042911Z","shell.execute_reply.started":"2025-10-07T17:12:35.027398Z","shell.execute_reply":"2025-10-07T17:12:35.042252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Selecting the most important features ","metadata":{}},{"cell_type":"code","source":"important_features = feature_importance_df[feature_importance_df['importance'] > 85]['feature'].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:12:38.768779Z","iopub.execute_input":"2025-10-07T17:12:38.769566Z","iopub.status.idle":"2025-10-07T17:12:38.774010Z","shell.execute_reply.started":"2025-10-07T17:12:38.769538Z","shell.execute_reply":"2025-10-07T17:12:38.773382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_selected = important_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:12:40.522349Z","iopub.execute_input":"2025-10-07T17:12:40.522644Z","iopub.status.idle":"2025-10-07T17:12:40.526469Z","shell.execute_reply.started":"2025-10-07T17:12:40.522623Z","shell.execute_reply":"2025-10-07T17:12:40.525713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Data preprocessing for sequences\n# Scaling features before reshaping\nscaler = StandardScaler()\ntrain[columns_selected] = scaler.fit_transform(train[columns_selected])\n\n# Reshaping data into sequences of 80 timesteps\nnum_breaths = len(train) // 80\nfeatures_reshaped = train[columns_selected].values.reshape(num_breaths, 80, -1)\ntargets_reshaped = target.values.reshape(num_breaths, 80, 1)\ngroups = train['breath_id'].unique()\n\n\n# PyTorch dataset for sequences \nclass VentilatorLSTMDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = torch.tensor(features, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n\n# LSTM model \nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_dim, hidden_dim, num_layers, \n            batch_first=True, dropout=0.2, bidirectional=True\n        )\n        self.fc = nn.Linear(hidden_dim*2, 1)\n\n    def forward(self, x):\n        # lstm_out shape: (batch_size, seq_len, hidden_dim * 2)\n        lstm_out, _ = self.lstm(x)\n        # Pass the output of every timestep to the final linear layer\n        predictions = self.fc(lstm_out)\n        return predictions\n\nn_splits = 2\nscores = []\nfolds = GroupKFold(n_splits=n_splits)\n\nBATCH_SIZE = 64 \nEPOCHS = 10\nLEARNING_RATE = 1e-3\nPATIENCE = 10\n\nfor fold, (train_idx, val_idx) in enumerate(folds.split(features_reshaped, targets_reshaped, groups=groups)):\n    print(f\"--- Fold {fold} ---\")\n    \n    # Splitting reshaped data\n    X_train, y_train = features_reshaped[train_idx], targets_reshaped[train_idx]\n    X_val, y_val = features_reshaped[val_idx], targets_reshaped[val_idx]\n\n    # datasets and dataLoaders\n    train_dataset = VentilatorLSTMDataset(X_train, y_train)\n    val_dataset = VentilatorLSTMDataset(X_val, y_val)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n    \n    # model, loss and optimizer\n    model = LSTMModel(input_dim=X_train.shape[2]).to(device)\n    criterion = nn.L1Loss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train_losses_fold = []\n    val_losses_fold = []\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss = 0.0\n        for features, labels in train_loader:\n            features, labels = features.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for features, labels in val_loader:\n                features, labels = features.to(device), labels.to(device)\n                outputs = model(features)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n\n        train_losses_fold.append(avg_train_loss)\n        val_losses_fold.append(avg_val_loss)\n        \n        print(f\"Epoch {epoch+1}/{EPOCHS}, Train MAE: {avg_train_loss:.4f}, Val MAE: {avg_val_loss:.4f}\")\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                print(\"Early stopping triggered.\")\n                break\n\n    scores.append(best_val_loss)\n    print(f\"Fold {fold} Best MAE: {best_val_loss:.4f}\\n\")\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses_fold, label='Training Loss')\n    plt.plot(val_losses_fold, label='Validation Loss')\n    plt.title(f'Fold {fold} - Training & Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss (MAE)')\n    plt.legend()\n    plt.grid(True)\n    epochs_run = len(train_losses_fold)\n    plt.xticks(np.arange(epochs_run), np.arange(1, epochs_run + 1))\n    plt.show()\n    \n    del model, X_train, y_train, X_val, y_val, train_loader, val_loader\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(f\"Average CV MAE with PyTorch LSTM: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:38:16.949040Z","iopub.execute_input":"2025-10-07T17:38:16.950000Z","iopub.status.idle":"2025-10-07T17:42:37.178795Z","shell.execute_reply.started":"2025-10-07T17:38:16.949966Z","shell.execute_reply":"2025-10-07T17:42:37.177670Z"}},"outputs":[],"execution_count":null}]}